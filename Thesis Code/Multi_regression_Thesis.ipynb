{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# evaluation metrics\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_final_updated_date.csv\", encoding = \"ISO-8859-1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN              DATATYPE       EXAMPLE\n",
      "----------------------------------------------------------------------\n",
      "Race_ID             int64          2163\n",
      "Trap                int64          2\n",
      "Odds                float64          5.5\n",
      "BSP                 float64          8.2\n",
      "Public_Estimate     int64          5\n",
      "Last_Run            int64          4\n",
      "Distance_All        float64          387.11\n",
      "Finish_All          float64          3.61\n",
      "Distance_Places_All  float64          380.0\n",
      "Races_All           int64          45\n",
      "Distance_Recent     float64          425.71\n",
      "Finish_Recent       float64          2.92\n",
      "Odds_Recent         float64          3.88\n",
      "Early_Recent        float64          2.77\n",
      "Races_380           int64          34\n",
      "Wins_380            float64          0.24\n",
      "Finish_380          float64          2.57\n",
      "Odds_380            float64          4.68\n",
      "Early_380           float64          2.43\n",
      "Grade_380           float64          7.86\n",
      "Time_380            float64          24.31\n",
      "Early_Time_380      float64          3.7\n",
      "Stay_380            float64          0.14\n",
      "Favourite           float64          1.0\n",
      "Finished            int64          4\n",
      "Wide_380            float64          0.0\n",
      "Dist_By             float64          -1.57\n",
      "Winner              int64          0\n",
      "Gng                 int64          -20\n",
      "Date                object          29 09 2015\n"
     ]
    }
   ],
   "source": [
    "print(\"COLUMN              DATATYPE       EXAMPLE\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "for col in df.columns:\n",
    "    print(col,\" \"*(18-len(col)),df[col].dtype,\" \"*(8-len(df[col].dtype)),df.iloc[8925][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13842 entries, 0 to 13841\n",
      "Data columns (total 30 columns):\n",
      "Race_ID                13842 non-null int64\n",
      "Trap                   13842 non-null int64\n",
      "Odds                   13842 non-null float64\n",
      "BSP                    13842 non-null float64\n",
      "Public_Estimate        13842 non-null int64\n",
      "Last_Run               13842 non-null int64\n",
      "Distance_All           13842 non-null float64\n",
      "Finish_All             13842 non-null float64\n",
      "Distance_Places_All    13842 non-null float64\n",
      "Races_All              13842 non-null int64\n",
      "Distance_Recent        13842 non-null float64\n",
      "Finish_Recent          13842 non-null float64\n",
      "Odds_Recent            13842 non-null float64\n",
      "Early_Recent           13842 non-null float64\n",
      "Races_380              13842 non-null int64\n",
      "Wins_380               13842 non-null float64\n",
      "Finish_380             13842 non-null float64\n",
      "Odds_380               13842 non-null float64\n",
      "Early_380              13842 non-null float64\n",
      "Grade_380              13842 non-null float64\n",
      "Time_380               13842 non-null float64\n",
      "Early_Time_380         13842 non-null float64\n",
      "Stay_380               13842 non-null float64\n",
      "Favourite              13842 non-null float64\n",
      "Finished               13842 non-null int64\n",
      "Wide_380               13842 non-null float64\n",
      "Dist_By                13842 non-null float64\n",
      "Winner                 13842 non-null int64\n",
      "Gng                    13842 non-null int64\n",
      "Date                   13842 non-null object\n",
      "dtypes: float64(20), int64(9), object(1)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['Race_ID','Trap','Odds','BSP','Public_Estimate','Last_Run','Distance_All','Finish_All',\n",
    "'Distance_Places_All','Races_All','Distance_Recent','Finish_Recent','Odds_Recent','Early_Recent',\n",
    "'Races_380','Wins_380','Finish_380','Odds_380','Early_380','Grade_380','Time_380','Early_Time_380',\n",
    "'Stay_380','Favourite','Finished','Wide_380','Dist_By','Gng','Winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature           | Pearson R   | P-value\n",
      "- - - - - - - - - - - - - - - - - - - - - - \n",
      "Finished              1.0       0.0\n",
      "Winner                -0.65       0.0\n",
      "BSP                   0.23       0.0\n",
      "Public_Estimate       0.22       0.0\n",
      "Odds                  0.21       0.0\n",
      "Finish_All            0.07       0.0\n",
      "Odds_380              0.06       0.0\n",
      "Odds_Recent           0.06       0.0\n",
      "Finish_380            0.06       0.0\n",
      "Finish_Recent         0.06       0.0\n",
      "Dist_By               -0.04       0.0\n",
      "Stay_380              0.04       0.0\n",
      "Last_Run              0.04       0.0\n",
      "Distance_Recent       0.03       0.0\n",
      "Early_Time_380        0.03       0.0\n",
      "Distance_All          0.02       0.02\n",
      "Distance_Places_All   0.02       0.05\n",
      "Trap                  0.01       0.19\n",
      "Races_All             0.01       0.19\n",
      "Gng                   -0.01       0.26\n",
      "Grade_380             0.01       0.33\n",
      "Races_380             0.01       0.33\n",
      "Wins_380              0.01       0.34\n",
      "Time_380              -0.0       0.62\n",
      "Early_380             -0.0       0.63\n",
      "Early_Recent          0.0       0.83\n",
      "Wide_380              -0.0       0.98\n",
      "Favourite             -0.0       0.99\n",
      "Race_ID               0.0       0.99\n"
     ]
    }
   ],
   "source": [
    "r_p = [] # Create a list of tuples with column names, r^2 and p-value. Then print them in order by p-value\n",
    "for x in factors:\n",
    "    result = pearsonr(df[x],df['Finished'])\n",
    "    pearson_r, p__value = result[0] , result[1]\n",
    "    r_p.append((x,pearson_r,p__value))\n",
    "cor_p_val = sorted(r_p, key=lambda x: x[2])\n",
    "print(\"Feature           | Pearson R   | P-value\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - \")\n",
    "for tup in cor_p_val:\n",
    "    print(tup[0],(20 - len(tup[0]))*\" \",round(tup[1],2),\"     \",round(tup[2],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validate = df[0:11670]# Training and validation\n",
    "df_test = df[11670:]# Final Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Winner','Public_Estimate','Odds','Race_ID','Distance_Places_All','Trap',\n",
    "             'Races_All','Grade_380','Races_380','Wins_380','Time_380','Early_380',\n",
    "             'Early_Recent','Wide_380','Favourite','Date']\n",
    "df_m = df_train_validate.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 9870\n",
    "train = df_m[0:train_size:]\n",
    "validation = df_m[train_size:]\n",
    "validation_all_features = df_train_validate[train_size:]\n",
    "target=\"Finished\"\n",
    "\n",
    "train_X = train.drop(columns=[target])\n",
    "train_y = train[target]\n",
    "validation_X = validation.drop(columns=[target])\n",
    "validation_y = validation[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.23\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='sag',\n",
    "                           max_iter=10000,multi_class='multinomial')\n",
    "\n",
    "model.fit(train_X,train_y)\n",
    "print(\"Training Accuracy\",round(accuracy_score(train_y, model.predict(train_X)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.231\n"
     ]
    }
   ],
   "source": [
    "predicted_fin = model.predict(validation_X)\n",
    "print(\"Validation Accuracy\",round(accuracy_score(validation_y, predicted_fin),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_y = list(validation_all_features['Public_Estimate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Accuracy 0.211\n"
     ]
    }
   ],
   "source": [
    "print(\"Public Accuracy\",round(accuracy_score(validation_y,public_y),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared Score/Error\n",
      "- - - - - - - - - - - - - - -\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-53eb14887f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"R-squared Score/Error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"- - - - - - - - - - - - - - -\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredicted_fin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Market:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpublic_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_y' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = df_test.drop(columns=[target])\n",
    "test_y = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.213\n"
     ]
    }
   ],
   "source": [
    "predicted_fin = model.predict(test_X)\n",
    "print(\"Test Accuracy\",round(accuracy_score(test_y, predicted_fin),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_y = list(df[11670:]['Public_Estimate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Accuracy 0.204\n"
     ]
    }
   ],
   "source": [
    "print(\"Public Accuracy\",round(accuracy_score(test_y,public_y),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Each Class\n",
      "- - - - - - - - - - - - - - - - - - - - \n",
      "First      0.56\n",
      "Second     0.13\n",
      "Third      0.09\n",
      "Fourth     0.04\n",
      "Fifth      0.09\n",
      "Sixth      0.38\n"
     ]
    }
   ],
   "source": [
    "matrix1 = confusion_matrix(test_y, predicted_fin)\n",
    "labls = [\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Sixth\"]\n",
    "acc_scores = [round(x,2) for x in list(matrix1.diagonal()/matrix1.sum(axis=1))]\n",
    "accs = zip(labls,acc_scores)\n",
    "print(\"Accuracy Score Each Class\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - \")\n",
    "for a in accs:\n",
    "    print(a[0],\" \"*(9-len(a[0])),a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[201  38  24  13  27  59]\n",
      " [155  47  26  21  30  83]\n",
      " [152  36  31  17  38  90]\n",
      " [139  34  28  16  39 104]\n",
      " [150  27  36  13  31 105]\n",
      " [113  19  33  22  39 136]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Each Class\n",
      "- - - - - - - - - - - - - - - - - - - - \n",
      "First      0.28\n",
      "Second     0.19\n",
      "Third      0.17\n",
      "Fourth     0.16\n",
      "Fifth      0.18\n",
      "Sixth      0.25\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(test_y, public_y)  ## MARKET ACCURACY \n",
    "labls = [\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Sixth\"]\n",
    "acc_scores = [round(x,2) for x in list(matrix.diagonal()/matrix.sum(axis=1))]\n",
    "accs = zip(labls,acc_scores)\n",
    "print(\"Accuracy Score Each Class\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - \")\n",
    "for a in accs:\n",
    "    print(a[0],\" \"*(9-len(a[0])),a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101  74  66  48  38  35]\n",
      " [ 71  70  53  59  50  59]\n",
      " [ 57  58  63  67  62  57]\n",
      " [ 46  64  56  56  79  59]\n",
      " [ 51  55  61  66  64  65]\n",
      " [ 28  52  52  80  60  90]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared Score/Error\n",
      "- - - - - - - - - - - - - - -\n",
      "Model:  -1.206\n",
      "Market: -0.615\n"
     ]
    }
   ],
   "source": [
    "print(\"R-squared Score/Error\")\n",
    "print(\"- - - - - - - - - - - - - - -\")\n",
    "print(\"Model: \",round(r2_score(test_y,predicted_fin),3))\n",
    "print(\"Market:\",round(r2_score(test_y,public_y),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
